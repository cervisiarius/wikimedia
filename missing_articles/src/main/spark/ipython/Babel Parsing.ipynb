{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Babel Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "babelreg = re.compile('{{Babel(.*)}}')\n",
    "\n",
    "def process_element(elem, lang):\n",
    "    langreg = '%s-(.)' % lang\n",
    "    if elem[1].text == '2':\n",
    "        text = elem[-1][-2].text\n",
    "        match = None\n",
    "        if text is not None:\n",
    "            match = babelreg.search (text)\n",
    "        if match is not None:\n",
    "            for g in match.groups():\n",
    "                match = re.search(langreg, g)\n",
    "                if match is not None:\n",
    "                    user =  elem[0].text.split(':')[1]\n",
    "                    proficiency = match.groups()[0]\n",
    "                    return (user, proficiency )\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_babel(dump, lang):\n",
    "    context = etree.iterparse( dump , tag = '{http://www.mediawiki.org/xml/export-0.10/}page')\n",
    "    l = []\n",
    "    for event, elem in context:\n",
    "        user = process_element(elem, lang)\n",
    "        if user is not None:\n",
    "            l.append(user)\n",
    "        elem.clear()\n",
    "        for ancestor in elem.xpath('ancestor-or-self::*'):\n",
    "            while ancestor.getprevious() is not None:\n",
    "                del ancestor.getparent()[0]\n",
    "    del context\n",
    "    return l\n",
    "\n",
    "def worker(t):\n",
    "    return find_babel(t[0], t[1])\n",
    "\n",
    "def find_babel_parallel(wikilang, babellang, n):\n",
    "    exp_dir = 'en-%s' % wikilang\n",
    "    args = [('/home/ellery/%s/dump%d.xml' % (exp_dir, i), babellang) for i in range(1, n+1)]\n",
    "    p = multiprocessing.Pool(8)\n",
    "    results = p.map(worker, args)\n",
    "    l = list(itertools.chain(*results))\n",
    "    df = pd.DataFrame(l)\n",
    "    df.columns = ['user', 'proficiency']\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import itertools\n",
    "\n",
    "s = 'en'\n",
    "t = 'es'\n",
    "min_date = '20140600000000'\n",
    "min_bytes = 100.0\n",
    "max_edit_history = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_users = find_babel_parallel(t,s,4)\n",
    "t_users = set(t_users[t_users['proficiency'] > '1']['user'])\n",
    "\n",
    "#WE DON\"T WANT EN USERS AFTERALL\n",
    "#s_users = find_babel_parallel(s,t, 26)\n",
    "#s_users = set(s_users[s_users['proficiency'] > '1']['user'])\n",
    "#s_users = s_users.difference(t_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1045\n"
     ]
    }
   ],
   "source": [
    "print len(t_users)\n",
    "#print len(s_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Overlapping Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ellery/wikimedia/missing_articles/missing_articles.ini']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ConfigParser import SafeConfigParser\n",
    "import os,sys,inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "from util import save_rdd, get_parser\n",
    "import json\n",
    "\n",
    "config = '/home/ellery/wikimedia/missing_articles/missing_articles.ini'\n",
    "cp = SafeConfigParser()\n",
    "cp.read(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find editors who have edited both source and target\n",
    "source_contributions_file = os.path.join(cp.get('general', 'contributions_dir'), s)\n",
    "target_contributions_file = os.path.join(cp.get('general', 'contributions_dir'), t)\n",
    "\n",
    "names = [\"language_code\", \"user_id\", \"user\", \"id\",\"page_title\",\"num_edits\",\"timestamp\", \"bytes_added\"]\n",
    "\n",
    "recent_source_users = sc.textFile(source_contributions_file)\\\n",
    ".map(get_parser(names))\\\n",
    ".filter(lambda x: len(x) == len(names))\\\n",
    ".filter(lambda x: x['timestamp'] > min_date)\\\n",
    ".map(lambda x: x['user'])\\\n",
    ".distinct().collect()\n",
    "\n",
    "source_users = sc.textFile(source_contributions_file)\\\n",
    ".map(get_parser(names))\\\n",
    ".filter(lambda x: len(x) == len(names))\\\n",
    ".map(lambda x: x['user'])\\\n",
    ".distinct().collect()\n",
    "\n",
    "target_users = sc.textFile(target_contributions_file)\\\n",
    ".map(get_parser(names))\\\n",
    ".filter(lambda x: len(x) == len(names))\\\n",
    ".map(lambda x: x['user'])\\\n",
    ".distinct().collect()\n",
    "\n",
    "recent_target_users = sc.textFile(target_contributions_file)\\\n",
    ".map(get_parser(names))\\\n",
    ".filter(lambda x: len(x) == len(names))\\\n",
    ".filter(lambda x: x['timestamp'] > min_date)\\\n",
    ".map(lambda x: x['user'])\\\n",
    ".distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o_users = set(recent_source_users).intersection(set(target_users))\n",
    "o_users = o_users.union(set(recent_target_users).intersection(set(source_users)))\n",
    "o_users = o_users.difference(t_users)\n",
    "#o_users = o_users.difference(t_users.union(s_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28463\n"
     ]
    }
   ],
   "source": [
    "print len(o_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get User Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymysql\n",
    "\n",
    "def query_db(query, params):\n",
    "    conn = pymysql.connect(host = 'analytics-store.eqiad.wmnet', read_default_file=\"/etc/mysql/conf.d/analytics-research-client.cnf\")\n",
    "    cursor = conn.cursor(pymysql.cursors.DictCursor)\n",
    "    cursor.execute(query, params)\n",
    "    rows = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return mysql_to_pandas(rows)\n",
    "\n",
    "\n",
    "def mysql_to_pandas(dicts):\n",
    "    dmaster = {}\n",
    "    for d in dicts:\n",
    "        for k in d.keys():\n",
    "            if k not in dmaster:\n",
    "                dmaster[k] = []\n",
    "            \n",
    "            dmaster[k].append(d[k]) \n",
    "    return pd.DataFrame(dmaster)\n",
    "\n",
    "def decode(x):\n",
    "    try:\n",
    "         return x.decode('utf-8')\n",
    "    except:\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT user_name, user_email\n",
    "FROM %(lang)swiki.user \n",
    "WHERE  length(user_email) > 1 \n",
    "AND user_editcount > 0 \n",
    "AND user_email_authenticated is not NULL\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_email_df = query_db(query % {'lang':t}, {})\n",
    "t_email_df.rename(columns={'user_name':'user'}, inplace=True)\n",
    "t_email_df['user'] = t_email_df['user'].apply(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_email_df = query_db(query % {'lang':s}, {})\n",
    "s_email_df.rename(columns={'user_name':'user'}, inplace=True)\n",
    "s_email_df['user'] = s_email_df['user'].apply(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get Edit History and Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_str(t):\n",
    "    uid, contributions = t\n",
    "    contributions = list(contributions)\n",
    "    uname = contributions[0]['user']\n",
    "    contributions = [{k: d[k] for k in ('language_code', 'id', 'page_title', 'num_edits', 'timestamp', 'bytes_added')} for d in contributions]\n",
    "    contributions.sort(key = lambda x: x['timestamp'])\n",
    "    if contributions[-1]['timestamp'] < min_date:\n",
    "        return None\n",
    "    else: \n",
    "        obj = {'uid': uid, 'uname': uname, 'contributions': contributions[-max_edit_history:]}\n",
    "        return (uname, json.dumps(obj))\n",
    "\n",
    "def get_contributions(cp, langs, user_set):\n",
    "    contributions_file = os.path.join(cp.get('general', 'contributions_dir'), langs[0])\n",
    "    contributions = sc.textFile(contributions_file)\n",
    "    for lang in langs[1:]:\n",
    "        contributions_file = os.path.join(cp.get('general', 'contributions_dir'), lang)\n",
    "        contributions = contributions.union(sc.textFile(contributions_file))\n",
    "        \n",
    "    contributions = contributions.map(get_parser(names))\\\n",
    "    .filter(lambda x: len(x) == 8)\\\n",
    "    .filter(lambda x: x['user'] in user_set)\\\n",
    "    .filter(lambda x: float(x['bytes_added']) > min_bytes)\\\n",
    "    .map(lambda x: (x['user'], x)).groupByKey()\\\n",
    "    .map(to_str)\\\n",
    "    .filter(lambda x: x is not None)\\\n",
    "    .collect()\n",
    "    contributions = pd.DataFrame(contributions)\n",
    "    contributions.columns =['user', 'history']\n",
    "    return contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# eswiki babel\n",
    "t_users_df = get_contributions(cp, [t], t_users)\n",
    "t_users_df['user'] = t_users_df['user'].apply(decode)\n",
    "t_users_df = t_users_df.merge(t_email_df, how='inner', on='user', suffixes=('_x', '_y'), copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# enwiki babel\n",
    "#s_users_df = get_contributions(cp, [s], s_users)\n",
    "#print s_users_df.shape\n",
    "#s_users_df['user'] = s_users_df['user'].apply(decode)\n",
    "#s_users_df = s_users_df.merge(s_email_df, how='inner', on='user', suffixes=('_x', '_y'), copy=True)\n",
    "#print s_users_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# overlap\n",
    "o_users_df = get_contributions(cp, [s, t], o_users)\n",
    "o_users_df['user'] = o_users_df['user'].apply(decode)\n",
    "o_users_df = o_users_df.merge(s_email_df, how='left', on='user', copy=True)\n",
    "o_users_df = o_users_df.merge(t_email_df, how='left', on='user', suffixes=('_x', '_t'), copy=True)\n",
    "o_users_df = o_users_df[o_users_df['user_email_x'] == o_users_df['user_email_t']]\n",
    "del o_users_df['user_email_t']\n",
    "o_users_df.rename(columns={'user_email_x':'user_email'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_split = int(t_users_df.shape[0]/2.0)\n",
    "#s_split = int(s_users_df.shape[0]/2.0)\n",
    "o_split = int(o_users_df.shape[0]/2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_users_df1 = t_users_df[:t_split]\n",
    "t_users_df2 = t_users_df[t_split:]\n",
    "\n",
    "#s_users_df1 = s_users_df[:s_split]\n",
    "#s_users_df2 = s_users_df[s_split:]\n",
    "\n",
    "o_users_df1 = o_users_df[:o_split]\n",
    "o_users_df2 = o_users_df[o_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = pd.concat([t_users_df1, o_users_df1] )\n",
    "df2 = pd.concat([t_users_df2, o_users_df2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.to_csv('/home/ellery/en-%s/e1.tsv' % t,  sep = '\\t', encoding = 'utf8')\n",
    "df2.to_csv('/home/ellery/en-%s/e2.tsv' % t,  sep = '\\t', encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4325, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4325, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
