{
 "metadata": {
  "name": "",
  "signature": "sha256:8743f0e789c32d38fc638534b60e0d2aad2f49120fff9f47fb9baeccfe53d7ac"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from operator import add\n",
      "from pyspark import SparkContext\n",
      "from pyspark.mllib.feature import HashingTF\n",
      "from pyspark.mllib.feature import IDF\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.stem.porter import PorterStemmer\n",
      "import string\n",
      "import gensim\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_translated_articles(files):\n",
      "    articles = None\n",
      "    for lang, f in files.iteritems():\n",
      "        lang_articles = sc.textFile(\"/user/west1/wikipedia_plaintexts/\" + f).map(lambda line: line.split('\\t'))\n",
      "        lang_articles = lang_articles.filter(lambda x: len(x) ==4 and len(x[3]) > 0 and len(x[2]) == 0)\n",
      "        lang_articles = lang_articles.map(lambda x: ('|'.join([lang, x[0], x[1]]), x[3].split(' ')))\n",
      "        \n",
      "        if articles == None:\n",
      "            articles = lang_articles\n",
      "        else:\n",
      "            articles = articles.union(lang_articles)\n",
      "    return articles\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_word_id_mapping(articles, min_occurrences = 3, num_tokens = 100000):\n",
      "    words = articles.flatMap(lambda x: x[1])\\\n",
      "    .map(lambda x: (x, 1))\\\n",
      "    .reduceByKey(add)\\\n",
      "    .filter(lambda x: x[1] >= min_occurrences)\\\n",
      "    .top(num_tokens, key=lambda x: x[1])\n",
      "    \n",
      "    local_word_map = []\n",
      "    for w in words:\n",
      "        t = (w[0], len(local_word_map))\n",
      "        local_word_map.append(t)\n",
      "    return dict(local_word_map), sc.parallelize(local_word_map)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "punctuation = set(string.punctuation)\n",
      "nltk_stopwords = set(stopwords.words('english'))\n",
      "wikimarkup = set(['*nl*', '</ref>', '<ref>', '--', '``', \"''\", \"'s\", 'also', 'refer' , '**'])\n",
      "banned_words = punctuation.union(nltk_stopwords).union(wikimarkup)\n",
      "\n",
      "def stem_word_list(words):\n",
      "    stemmer = PorterStemmer()\n",
      "    return [stemmer.stem(w.lower()) for w in words]\n",
      "    \n",
      "def clean_article_text(articles):\n",
      "    lower_articles = articles.map(lambda x: (x[0], [w.lower() for w in x[1]]))\n",
      "    # remove banned words\n",
      "    clean_articles = lower_articles.map(lambda x: (x[0], [w for w in x[1] if w not in banned_words]))\n",
      "    # Stem\n",
      "    #stemmed_articles = clean_articles.map(lambda x: (x[0], stem_word_list(x[1])))\n",
      "    return clean_articles #stemmed_articles\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_term_frequencies(articles):\n",
      "    def map_tokens_to_frequencies(x):\n",
      "        doc_id, tokens = x\n",
      "        from collections import Counter\n",
      "        frequencies = list(Counter(tokens).items())\n",
      "        return (doc_id, frequencies)  \n",
      "    return articles.map(map_tokens_to_frequencies)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def translate_words_to_ids(tf_articles):\n",
      "    def flatten(x):\n",
      "        article_id, counts = x\n",
      "        elems = [(c[0], (article_id, c[1])) for c in counts]\n",
      "        return elems\n",
      "\n",
      "    def map_tokens_to_ids(x):\n",
      "        (u'Lajoie', ((u'simple|September_5|9948', 1), 94944))\n",
      "        (token, ((doc_id, count), token_id)) = x\n",
      "        return (doc_id, (token_id, count))\n",
      "\n",
      "    return tf_articles.flatMap(flatten).\\\n",
      "    join(word_id_map).\\\n",
      "    map(map_tokens_to_ids).\\\n",
      "    groupByKey()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files = { 'simple': 'simplewiki-20150406', 'en': 'enwiki-20150304'}\n",
      "min_occurrences = 3\n",
      "num_tokens = 50000\n",
      "\n",
      "# load artilces\n",
      "tokenized_articles = load_translated_articles(files)\n",
      "# normalize\n",
      "tokenized_articles = clean_article_text(tokenized_articles)\n",
      "# get word-id maping\n",
      "local_word_id_map, word_id_map = get_word_id_mapping(tokenized_articles, min_occurrences = min_occurrences, num_tokens = num_tokens)\n",
      "local_id_word_map = inv_map = {v: k for k, v in local_word_id_map.items()}\n",
      "# map list of tokens into list of token:count elements\n",
      "tf_articles = get_term_frequencies(tokenized_articles)\n",
      "# map list of token:count elements into list of token-id:count\n",
      "id_articles = translate_words_to_ids(tf_articles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "home_dir = '/home/ellery/'\n",
      "hadoop_home_dir = '/user/ellery/'\n",
      "exp_dir = 'simple_lda/'\n",
      "base_dir = home_dir + exp_dir\n",
      "hadoop_base_dir = hadoop_home_dir + exp_dir\n",
      "\n",
      "dict_file = 'dictionary.txt'\n",
      "\n",
      "zip_dir = 'articles.pre_blei_dir'\n",
      "zip_file = 'articles.pre_blei.gz'\n",
      "combined_file = 'articles.pre_blei'\n",
      "\n",
      "article_name_file = 'articles.txt'\n",
      "article_file = 'articles.blei'\n",
      "article_vectors_file = 'article_vectors.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print os.system( 'rm -r ' + base_dir)\n",
      "print os.system( 'mkdir ' + base_dir)\n",
      "print os.system( 'hadoop fs -rm -r ' + hadoop_base_dir)\n",
      "print os.system( 'hadoop fs -mkdir ' + hadoop_base_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "0\n",
        "256"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save dictionary to file: word at line i has id i\n",
      "with open(base_dir + dict_file, 'w') as f:\n",
      "    for word, word_id in sorted(local_word_id_map.items(), key=lambda x:x[1]):\n",
      "        line = word + '\\n'\n",
      "        f.write(line.encode('utf8'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save rdd to file\n",
      "def tuple_to_str(t):\n",
      "    article, vector = t\n",
      "    str_vector = [str(e[0]) + ':' + str(e[1]) for e in vector]\n",
      "    return article + ' ' + str(len(str_vector)) + ' ' + ' '.join(str_vector)\n",
      "id_articles.map(tuple_to_str).saveAsTextFile (\"hdfs://\" + hadoop_base_dir + zip_dir, compressionCodecClass  =  \"org.apache.hadoop.io.compress.GzipCodec\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print os.system('hadoop fs -copyToLocal ' + hadoop_base_dir + zip_dir + ' ' + base_dir +  zip_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n"
       ]
      }
     ],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print os.system( 'cat ' + base_dir + zip_dir + '/*.gz > ' + base_dir + zip_file)\n",
      "print os.system( 'gunzip ' + base_dir + zip_file)\n",
      "print os.system( \"cut  -d' ' -f2- \" + base_dir + combined_file + ' > ' + base_dir + article_file)\n",
      "print os.system( \"cut  -d' ' -f1 \" + base_dir + combined_file + ' > ' + base_dir + article_name_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import gensim\n",
      "dictionary = gensim.corpora.dictionary.Dictionary() \n",
      "id2Token = dict(enumerate(l[:-1] for l in open(base_dir + dict_file))) \n",
      "dictionary.token2id  = {v: k for k, v in id2Token.items()}\n",
      "corpus = gensim.corpora.bleicorpus.BleiCorpus(base_dir + article_file, fname_vocab= base_dir + dict_file ) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "#model =  gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=10)\n",
      "time1 = time.time()\n",
      "model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\\\n",
      "                            num_topics=100,\\\n",
      "                            id2word=dictionary,\\\n",
      "                            workers=4,\\\n",
      "                            chunksize=2000,\\\n",
      "                            passes=1,\\\n",
      "                            batch=False,\\\n",
      "                            alpha='symmetric',\\\n",
      "                            eta=None,\\\n",
      "                            decay=0.5,\\\n",
      "                            offset=1.0,\\\n",
      "                            eval_every=10,\\\n",
      "                            iterations=50,\\\n",
      "                            gamma_threshold=0.001)\n",
      "time2 = time.time()\n",
      "print 'function took %0.3f minutes' % ((time2-time1) / 60.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "function took 2.766 minutes\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matrix = model[corpus]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.matutils import MmWriter\n",
      "MmWriter.write_corpus(os.path.join(base_dir, article_vectors_file), matrix, progress_cnt=1000)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for doc in matrix:\n",
      "    print doc\n",
      "    break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(14, 0.90099999999548852)]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "'TransformedCorpus' object does not support indexing",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-00f086116721>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: 'TransformedCorpus' object does not support indexing"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model[corpus.docbyoffset (0)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 159,
       "text": [
        "[(46, 0.064157640640683966),\n",
        " (59, 0.14757521897828216),\n",
        " (60, 0.73120831685161869)]"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpus.docbyoffset (0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 156,
       "text": [
        "[(146, 1.0),\n",
        " (190, 1.0),\n",
        " (125, 1.0),\n",
        " (20, 1.0),\n",
        " (962, 1.0),\n",
        " (3, 1.0),\n",
        " (112, 1.0),\n",
        " (304, 1.0),\n",
        " (97, 1.0),\n",
        " (1919, 1.0),\n",
        " (277, 1.0),\n",
        " (28586, 1.0),\n",
        " (113, 1.0),\n",
        " (118, 1.0),\n",
        " (7278, 1.0),\n",
        " (4938, 1.0)]"
       ]
      }
     ],
     "prompt_number": 156
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.print_topic(60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 163,
       "text": [
        "'0.009*german + 0.009*die + 0.008*american + 0.006*born + 0.006*april + 0.005*first + 0.005*march + 0.005*war + 0.005*b. + 0.005*octob'"
       ]
      }
     ],
     "prompt_number": 163
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.print_topic(23)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 105,
       "text": [
        "'0.019*american + 0.017*born + 0.009*die + 0.008*januari + 0.008*februari + 0.008*april + 0.007*juli + 0.007*march + 0.007*actor + 0.007*novemb'"
       ]
      }
     ],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "model.print_topic(72)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 167,
       "text": [
        "'0.036*name + 0.035*ref + 0.012*born + 0.007*refer + 0.007*unit + 0.007*american + 0.006*presid + 0.006*state + 0.006*new + 0.005*first'"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_doc_vectors_to_file(model, corpus):\n",
      "    # write reduced document vectors to file\n",
      "    article_name_f = open(base_dir + article_name_file, 'r')\n",
      "    article_f = open(base_dir + article_file, 'r')\n",
      "    article_vectors_f = open(base_dir + article_vectors_file, 'w')\n",
      "\n",
      "    time1 = time.time()\n",
      "    for i in range(len(corpus)):\n",
      "        doc = [ p.split(':') for p in next(article_f)[:-1].split(' ')[1:]]\n",
      "        doc = [(int(p[0]), int(p[1])) for p in doc]\n",
      "        line = next(article_name_f).strip() + ' '\n",
      "        v = model[doc]\n",
      "        str_v = [ str(topic_id)+':'+str(weight) for topic_id, weight in v ]\n",
      "        line += ' '.join(str_v) + '\\n'\n",
      "        article_vectors_f.write(line)\n",
      "    if i % 10000 == 0:\n",
      "        print line\n",
      "    time2 = time.time()\n",
      "    print 'function took %0.3f minutes' % ((time2-time1) / 60.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 164
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "write_doc_vectors_to_file(model, corpus)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "function took 5.345 minutes\n"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# move document vectors to hdfs\n",
      "print os.system('hadoop fs -mkdir ' + hadoop_base_dir )\n",
      "print os.system('hadoop fs -put ' + base_dir + article_vectors_file + ' ' + hadoop_base_dir + article_vectors_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute editor interest vectors and save\n",
      "\n",
      "from pyspark.mllib.linalg import Vectors\n",
      "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])\n",
      "sv2 = Vectors.sparse(2, [0, 2], [1.0, 3.0])\n",
      "sv1.add(sv2)\n",
      "\n",
      "# implement recommender "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "'SparseVector' object has no attribute 'add'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-181-6d08df601595>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msv2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0msv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msv2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# implement recommender\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAttributeError\u001b[0m: 'SparseVector' object has no attribute 'add'"
       ]
      }
     ],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print os.system('hadoop fs -put ' +  base_dir + article_vectors_file + ' ' + hadoop_base_dir + article_vectors_file)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0\n"
       ]
      }
     ],
     "prompt_number": 179
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'hadoop fs -put ' +  base_dir + article_vectors_file + ' ' + hadoop_base_dir + article_vectors_file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 178,
       "text": [
        "'hadoop fs -put /home/ellery/tmp/article_vectors.txt /user/ellery/tmp/article_vectors.txt'"
       ]
      }
     ],
     "prompt_number": 178
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def save_and_transform_tfidf(articles, minDocFreq = 3):\n",
      "    # keeping the docids and TFIDF vectors in sync seems sketchy but\n",
      "    #https://issues.apache.org/jira/browse/SPARK-6340 says this works\n",
      "    doc_ids = articles.map(lambda x: x[0])\n",
      "    doc_tokens1 = articles.map(lambda x: x[1])\n",
      "    tf = HashingTF(numFeatures=1048576).transform(doc_tokens1)\n",
      "    tf.cache()\n",
      "    idf = IDF(minDocFreq=minDocFreq).fit(tf)\n",
      "    tfidf = idf.transform(tf)\n",
      "    return  doc_ids.zip(tfidf)\n",
      "     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cleaned_articles = clean_article_text(tokenized_articles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "articles = transform_tfidf(cleaned_articles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# save as gzip\n",
      "# figure out how to feed to gensim\n",
      "# get topic vectors\n",
      "# implement recommendation algorithm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_word_counts(articles, min_occurrences = 3, num_tokens = 10000):\n",
      "    words = articles.flatMap(lambda x: x[1])\\\n",
      "    .map(lambda x: (x, 1))\\\n",
      "    .reduceByKey(add)\\\n",
      "    .filter(lambda x: x[1] >= min_occurrences)\\\n",
      "    .top(num_tokens, key=lambda x: x[1])\n",
      "    return words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_id_map = get_word_counts(tokenized_articles)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_id_map[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "(u'gathering', 168)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}