{
 "metadata": {
  "name": "",
  "signature": "sha256:0c1836a350adca2b7ceb06e98004c76b426c7a89baafff69bab0152fcf7a1e6f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import time"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "home_dir = '/home/ellery'\n",
      "hadoop_home_dir = '/user/ellery'\n",
      "exp_dir = 'en-simple'\n",
      "base_dir = os.path.join(home_dir, exp_dir)\n",
      "hadoop_base_dir = os.path.join(hadoop_home_dir,  exp_dir)\n",
      "article_vectors_file = 'article_vectors.txt'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_pair(p):\n",
      "        index, value  = p.split(':')\n",
      "        return int(index), float(value)\n",
      "    \n",
      "def parse_vector_text(line):\n",
      "    tokens = line.strip().split(' ')\n",
      "    article_id = '|'.join(tokens[0].split('|')[:2])\n",
      "    d = dict(parse_pair(p) for p in tokens[1:])\n",
      "    return (article_id, d)\n",
      "\n",
      "sparse_vectors = sc.textFile(os.path.join(hadoop_base_dir,article_vectors_file )).map(parse_vector_text)\n",
      "sparse_vectors.cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "PythonRDD[10] at RDD at PythonRDD.scala:42"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sparse_vectors.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[(u'en|Para_(community_development_block)',\n",
        "  {22: 0.0832122824336,\n",
        "   36: 0.0320615456931,\n",
        "   55: 0.0118875317921,\n",
        "   60: 0.021826188184,\n",
        "   110: 0.0214627328078,\n",
        "   124: 0.0551688622094,\n",
        "   161: 0.0885485157704,\n",
        "   163: 0.010220344568,\n",
        "   182: 0.266302156801,\n",
        "   227: 0.0383211058322,\n",
        "   274: 0.0392821805583,\n",
        "   299: 0.210611283285,\n",
        "   321: 0.0114688280183,\n",
        "   348: 0.0160493340069,\n",
        "   369: 0.0142894527492,\n",
        "   370: 0.0492498188719}),\n",
        " (u'en|Jhaukhel', {126: 0.056139068087, 308: 0.888583154135}),\n",
        " (u'en|Channel_49_digital_TV_stations_in_the_United_States',\n",
        "  {1: 0.0256010078016,\n",
        "   3: 0.252387229603,\n",
        "   9: 0.0181346507219,\n",
        "   13: 0.0694249900829,\n",
        "   19: 0.0100438981969,\n",
        "   31: 0.0223808241986,\n",
        "   52: 0.0220143790635,\n",
        "   81: 0.0438393939978,\n",
        "   97: 0.0188157635913,\n",
        "   131: 0.0362270219515,\n",
        "   137: 0.149547664902,\n",
        "   161: 0.0106478643073,\n",
        "   170: 0.0366974573625,\n",
        "   183: 0.0357795366329,\n",
        "   232: 0.0236641774218,\n",
        "   244: 0.0246566494248,\n",
        "   260: 0.0570844306014,\n",
        "   275: 0.0159661482767,\n",
        "   357: 0.0177404455226}),\n",
        " (u'en|Vern_S._Williams',\n",
        "  {17: 0.0150870596973,\n",
        "   48: 0.0243954801183,\n",
        "   98: 0.0597485209047,\n",
        "   131: 0.0248931522912,\n",
        "   157: 0.062196897981,\n",
        "   169: 0.0110695583581,\n",
        "   173: 0.0254750985326,\n",
        "   178: 0.010093152578,\n",
        "   196: 0.0225418255191,\n",
        "   226: 0.0691517285089,\n",
        "   277: 0.122383776882,\n",
        "   290: 0.0312256493459,\n",
        "   306: 0.148131026243,\n",
        "   317: 0.108797314627,\n",
        "   337: 0.0948773541794,\n",
        "   346: 0.0203443536111,\n",
        "   384: 0.0593274093883}),\n",
        " (u'en|Trillium_ovatum',\n",
        "  {1: 0.0129145309769,\n",
        "   9: 0.0587428209785,\n",
        "   13: 0.0165496108359,\n",
        "   45: 0.393932946897,\n",
        "   83: 0.0321804676835,\n",
        "   102: 0.0119569362097,\n",
        "   112: 0.0251112922425,\n",
        "   131: 0.0361012630228,\n",
        "   137: 0.0136896262678,\n",
        "   193: 0.0117461744191,\n",
        "   197: 0.0147443095211,\n",
        "   260: 0.0188568129249,\n",
        "   299: 0.0336814002146,\n",
        "   327: 0.145123158234,\n",
        "   351: 0.11396340842,\n",
        "   354: 0.0497961502419}),\n",
        " (u'en|Marty_Wilde',\n",
        "  {29: 0.0372269396251,\n",
        "   46: 0.024092929628,\n",
        "   82: 0.0121369295642,\n",
        "   87: 0.0249422182598,\n",
        "   100: 0.0112816560538,\n",
        "   154: 0.100389082333,\n",
        "   168: 0.0141980377338,\n",
        "   187: 0.013177012181,\n",
        "   229: 0.0412092044422,\n",
        "   231: 0.212578539127,\n",
        "   240: 0.0344980064913,\n",
        "   251: 0.0702988124489,\n",
        "   259: 0.0945857840358,\n",
        "   261: 0.0682869145176,\n",
        "   346: 0.0342074398994,\n",
        "   348: 0.0594409182349,\n",
        "   353: 0.0383147018115}),\n",
        " (u'en|Duffryn_High_School',\n",
        "  {64: 0.0136896264308,\n",
        "   84: 0.124663949906,\n",
        "   125: 0.0300421537671,\n",
        "   158: 0.0329432385452,\n",
        "   277: 0.129312305559,\n",
        "   300: 0.0542197520792,\n",
        "   306: 0.274354729178,\n",
        "   341: 0.0115532953711,\n",
        "   354: 0.222428315038,\n",
        "   365: 0.036636957645,\n",
        "   388: 0.0135630871938}),\n",
        " (u'en|Calling_Lake,_Alberta',\n",
        "  {3: 0.271650716002,\n",
        "   36: 0.0340034964958,\n",
        "   83: 0.0909154578843,\n",
        "   126: 0.0169488500074,\n",
        "   150: 0.111315036157,\n",
        "   159: 0.0189352102588,\n",
        "   165: 0.0410517088955,\n",
        "   167: 0.0142132976052,\n",
        "   176: 0.0528862590397,\n",
        "   227: 0.0461072787841,\n",
        "   235: 0.0104388885481,\n",
        "   297: 0.034045132897,\n",
        "   308: 0.084957735785,\n",
        "   318: 0.0118670224895,\n",
        "   347: 0.0238415830773,\n",
        "   370: 0.105686775972,\n",
        "   374: 0.0109348148039}),\n",
        " (u'en|Craig_Jackson_(swimmer)',\n",
        "  {100: 0.0314678912876,\n",
        "   118: 0.740562867173,\n",
        "   187: 0.0172503789944,\n",
        "   234: 0.0212000496922,\n",
        "   239: 0.0544445045209,\n",
        "   338: 0.021551576533,\n",
        "   342: 0.0938721065465}),\n",
        " (u'en|Ken_Duggan',\n",
        "  {93: 0.0561582649051,\n",
        "   159: 0.0554141477032,\n",
        "   201: 0.630986688905,\n",
        "   281: 0.166313006061,\n",
        "   348: 0.0283519724902,\n",
        "   356: 0.0292455298356})]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sparse_l2(d1, d2):\n",
      "    print d1, d2\n",
      "    common_dims = d1.viewkeys() & d2.viewkeys()\n",
      "    d1_dims = d1.viewkeys() - common_dims\n",
      "    d2_dims = d2.viewkeys() - common_dims\n",
      "    l2 = 0.0\n",
      "    l2 += sum([(d1[k] -d2[k])**2 for k in common_dims])\n",
      "    l2 += sum([d1[k]**2 for k in d1_dims])\n",
      "    l2 += sum([d2[k]**2 for k in d2_dims])\n",
      "    return l2   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_nn(article_id, topic_dict, sparse_vectors):\n",
      "    def get_dist(t):\n",
      "        article_id, d1 = t\n",
      "        return article_id, sparse_l2(d1, topic_dict)\n",
      "    \n",
      "    return sparse_vectors.map(get_dist).top(25, key = lambda x: -x[1])  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "article_id = u'en|Biology'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topic_dict = sparse_vectors.lookup(article_id)[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_nn(article_id, topic_dict, sparse_vectors )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "[(u'en|Biology', 0.0),\n",
        " (u'en|Inheritance_of_acquired_characteristics', 0.014561350066469164),\n",
        " (u'en|Modern_evolutionary_synthesis', 0.015086984124311909),\n",
        " (u'en|Green-beard_effect', 0.017422712788819406),\n",
        " (u'en|Genetics_and_the_Origin_of_Species', 0.017517616141411216),\n",
        " (u'en|Evolutionary_developmental_biology', 0.01872683891909423),\n",
        " (u'en|Evolutionary_physiology', 0.018736029266364501),\n",
        " (u'en|Zoology', 0.020321635929253824),\n",
        " (u'en|Lamarckism', 0.020654335141788788),\n",
        " (u'en|Outline_of_biology', 0.020912482369577906),\n",
        " (u'en|Imre_Festetics', 0.023025303452462752),\n",
        " (u'en|Saltation_(biology)', 0.023698912038535212),\n",
        " (u'en|Organism', 0.026576256724288445),\n",
        " (u'en|Biology', 0.026635746677138514),\n",
        " (u'en|Adaptive_mutation', 0.026822607186596247),\n",
        " (u'en|Genetic_matchmaking', 0.029240116213807316),\n",
        " (u'en|Phenotypic_integration', 0.02949936601947091),\n",
        " (u'en|Cloning', 0.029925309846611806),\n",
        " (u'en|Shifting_balance_theory', 0.030534112435103851),\n",
        " (u'en|Fitness', 0.03088122103881228),\n",
        " (u'en|Macroevolution', 0.031117025071584502),\n",
        " (u'en|Articulata_hypothesis', 0.031164433539138281),\n",
        " (u'en|Genetics_of_aggression', 0.031274862965891818),\n",
        " (u'en|Unit_of_selection', 0.032277795083643625),\n",
        " (u'en|History_of_zoology_(since_1859)', 0.032619023736374257)]"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target = 'simple'\n",
      "import math\n",
      "\n",
      "def get_parser(names):\n",
      "    def loadRecord(line):\n",
      "        #input = StringIO.StringIO(line)\n",
      "        cells = line.strip().split('\\t')\n",
      "        return dict(zip(names, cells))\n",
      "    return loadRecord\n",
      "\n",
      "names = [\"language_code\", \"user_id\", \"user\", \"id\",\"page_title\",\"num_edits\",\"bytes_added\"]\n",
      "target_revision_histories = sc.textFile(\"/user/west1/revision_history_aggregated/\" + target).map(get_parser(names))\n",
      "\n",
      "target_ratings = target_revision_histories.map(lambda x: (int(x['user_id']), int(x['page_title']), float(x['bytes_added']))).groupByKey()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "target_ratings.take(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 17.0 failed 4 times, most recent failure: Lost task 9.3 in stage 17.0 (TID 122, analytics1019.eqiad.wmnet): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/h/yarn/local/usercache/ellery/filecache/3698/spark-assembly-1.3.0-hadoop2.4.0.jar/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/var/lib/hadoop/data/h/yarn/local/usercache/ellery/filecache/3698/spark-assembly-1.3.0-hadoop2.4.0.jar/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 2253, in pipeline_func\n  File \"/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 2253, in pipeline_func\n  File \"/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 270, in func\n  File \"/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 1706, in combineLocally\n  File \"/var/lib/hadoop/data/h/yarn/local/usercache/ellery/filecache/3698/spark-assembly-1.3.0-hadoop2.4.0.jar/pyspark/shuffle.py\", line 252, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-46-152d8862b7c5>\", line 14, in <lambda>\nValueError: invalid literal for int() with base 10: 'Endocrine_system'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:307)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-47-e196596c41cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtarget_ratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    841\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m         \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjavaPartitions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallowLocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_iterator_through_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/otto/spark-1.3.0-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[1;32m--> 538\u001b[1;33m                 self.target_id, self.name)\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/otto/spark-1.3.0-bin-hadoop2.4/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    299\u001b[0m                     \u001b[1;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[0;32m    301\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 9 in stage 17.0 failed 4 times, most recent failure: Lost task 9.3 in stage 17.0 (TID 122, analytics1019.eqiad.wmnet): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/var/lib/hadoop/data/h/yarn/local/usercache/ellery/filecache/3698/spark-assembly-1.3.0-hadoop2.4.0.jar/pyspark/worker.py\", line 101, in main\n    process()\n  File \"/var/lib/hadoop/data/h/yarn/local/usercache/ellery/filecache/3698/spark-assembly-1.3.0-hadoop2.4.0.jar/pyspark/worker.py\", line 96, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 2253, in pipeline_func\n  File \"/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 2253, in pipeline_func\n  File \"/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 270, in func\n  File \"/home/otto/spark-1.3.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 1706, in combineLocally\n  File \"/var/lib/hadoop/data/h/yarn/local/usercache/ellery/filecache/3698/spark-assembly-1.3.0-hadoop2.4.0.jar/pyspark/shuffle.py\", line 252, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-46-152d8862b7c5>\", line 14, in <lambda>\nValueError: invalid literal for int() with base 10: 'Endocrine_system'\n\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:135)\n\tat org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:176)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:94)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:307)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:244)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:64)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}