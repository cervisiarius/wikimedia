---
title: "Analysis of added links"
author: "Bob West"
date: "07/01/2015"
output: html_document
---

# Load the data and beat it into shape

```{r}
.default_par <- par(no.readonly=TRUE)

DATADIR <- sprintf('%s/wikimedia/trunk/data/link_placement/', Sys.getenv('HOME'))

data <- read.table(pipe(sprintf('gunzip -c %s/links_added_in_02-15_WITH-STATS.tsv.gz', DATADIR)),
                   header=TRUE, quote='', sep='\t')
data[,1] <- paste(data$src, data$tgt)
data <- data[,-2]
colnames(data)[1] <- 'cand'

data$src <- unlist(lapply(strsplit(data$cand, ' '), function(pair) pair[1]))
data$tgt <- unlist(lapply(strsplit(data$cand, ' '), function(pair) pair[2]))

data$num_wiki_searches_before <- data$num_wiki_searches_302_before + data$num_wiki_searches_200_before
data$num_wiki_searches_after <- data$num_wiki_searches_302_after + data$num_wiki_searches_200_after

# Status-302 searches are counted as direct clicks in the input data, so correct for this.
# Q: WHY is sum(data$num_wiki_searches_302_before > data$num_clicks_before) == 666, not 0?
# A: Maybe if the browser somehow fires several 302 search requests but only one 200 forward
# to the article.
data$num_clicks_before <- pmax(0, data$num_clicks_before - data$num_wiki_searches_302_before)
data$num_clicks_after <- pmax(0, data$num_clicks_after - data$num_wiki_searches_302_after)

# Consider only those source that have at least one pageview before.
data <- data[data$num_source_before > 0,]

# Remove those that still have direct clicks before for some reason.
data <- data[data$num_clicks_before == 0,]

# The p_st probabilities.
# NB: This is greater than 1 for 7 pairs, 4 of them involving the target "List_of_markets_in_South_Korea".
data$pst_after <- pmin(1, data$num_clicks_after / data$num_source_after)

# The indirect-link probabilities (excluding search and direct clicks).
data$p_indir_before <- pmax(0, pmin(1, (data$num_paths_before - data$num_wiki_searches_before -
                          data$num_external_searches_before) / data$num_source_before))
data$p_indir_after <- pmax(0, pmin(1, (data$num_paths_after - data$num_wiki_searches_after -
                         data$num_external_searches_after - data$num_clicks_after) / data$num_source_after))

# The direct-click probabilities (ratio direct clicks vs. all paths).
data$click_ratio_after <- data$num_clicks_after / data$num_paths_after

# The relative position of the first link occurrence in the article text.
data$rel_link_pos <- data$min_link_pos / data$source_length

data$num_all_searches_before <- data$num_external_searches_before + data$num_wiki_searches_before
data$num_all_searches_after <- data$num_external_searches_after + data$num_wiki_searches_after

# Compute the search probabilities.
data$search_prob_ext_before <- data$num_external_searches_before / data$num_source_before
# Path counts already include 302 searches, so we just need to add the 200 searches in the denominator.
data$search_prob_wiki_before <- data$num_wiki_searches_before / data$num_source_before
data$search_prob_all_before <- data$num_all_searches_before / data$num_source_before
data$search_prob_ext_after <- data$num_external_searches_after / data$num_source_after
# Path counts already include 302 searches, so we just need to add the 200 searches in the denominator.
data$search_prob_wiki_after <- data$num_wiki_searches_after / data$num_source_after
data$search_prob_all_after <- data$num_all_searches_after / data$num_source_after
```

# Popularity before vs. after

```{r}
before <- tapply(data$num_source_before, data$src, function(v) v[1])
after <- tapply(data$num_source_after, data$src, function(v) v[1])
after <- after[before > 0]
before <- before[before > 0]
boxplot(after-before)
grid()
summary((after-before)/before)
```

# Usage of newly added links

```{r}
# A scatter plot of the number of paths before vs. after (incl. direct clicks after).
# We jitter the data to spread the overlay at low values.
x <- jitter(data$num_paths_before[data$num_paths_before > 0 & data$num_paths_after > 0], amount=0.5)
y <- jitter(data$num_paths_after[data$num_paths_before > 0 & data$num_paths_after > 0], amount=0.5)
plot(x, y, col='#00000005', pch=16, log='xy', xlim=range(c(x,y)), ylim=range(c(x,y)),
     xlab='Num paths before', ylab='Num paths after (direct and indirect)')
abline(0,1, col='red')

# Now consider indirect paths only. Here, the data is much closer to the diagonal, which means that
# the increased volume in transitions from s to t comes mostly from direct link clicks.
# The number of indirect paths does not seem to decrease after the link is added.
x_raw <- data$num_paths_before
y_raw <- data$num_paths_after - data$num_clicks_after
x <- jitter(x_raw[x_raw > 0 & y_raw > 0], amount=0.5)
y <- jitter(y_raw[x_raw > 0 & y_raw > 0], amount=0.5)
plot(x, y, col='#00000005', pch=16, log='xy', xlim=range(c(x,y)), ylim=range(c(x,y)),
     xlab='Num paths before', ylab='Num paths after (indirect only)')
abline(0,1, col='red')

# Also subtract searches, and normalize by the number of visits to s.
plot(data$p_indir_before, data$p_indir_after, col='#00000005', pch=16, log='xy',
     xlim=c(1e-6, 1), ylim=c(1e-6, 1), cex=2,
     xlab='Fraction paths before', ylab='Fraction paths after (indirect only)')
abline(0,1, col='red')
# This boxplot shows that the prob of indirect paths changes. Grouping is by number of indir paths before.
groups_idx <- split(1:nrow(data), cut(data$p_indir_before * data$num_source_before,
                                      breaks=c(1,2,10,50,100,Inf), right=FALSE))
groups_val <- lapply(groups_idx,
                     function(idx) ((data$p_indir_after - data$p_indir_before)/data$p_indir_before)[idx])
boxplot(groups_val, notch=TRUE, names=names(groups_idx), outline=FALSE)
abline(h=0)
grid()

# Let's check out searches.
x_raw <- data$num_wiki_searches_before + data$num_external_searches_before
y_raw <- data$num_wiki_searches_after + data$num_external_searches_after
x <- jitter(x_raw[x_raw > 0 & y_raw > 0], amount=0.5)
y <- jitter(y_raw[x_raw > 0 & y_raw > 0], amount=0.5)
plot(x, y, col='#00000010', pch=16, log='xy', xlim=range(c(x,y)), ylim=range(c(x,y)),
     xlab='Num searches before', ylab='Num searches after')
abline(0,1, col='red')

# Normalized by p_s.
plot(data$search_prob_all_before, data$search_prob_all_after, col='#00000010', pch=16, log='xy',
     xlim=c(1e-6, 1e-1), ylim=c(1e-6, 1e-1), cex=2,
     xlab='Search prob before', ylab='Search prob after')
abline(0,1, col='red')
# This boxplot shows that the search prob changes. Grouping is by number of searches before.
groups_idx <- split(1:nrow(data), cut(data$num_all_searches_before$,
                                      breaks=c(1,2,10,30,Inf), right=FALSE))
groups_val <- lapply(groups_idx, function(idx)
  ((data$search_prob_all_after - data$search_prob_all_before)/data$search_prob_all_before)[idx])
boxplot(groups_val, notch=TRUE, names=names(groups_idx), outline=FALSE)
panel.first=abline(h=0)
grid()

# Most clicks are not used at all (66%); among the rest, there's a long tail, with only few links
# used frequently. Only 1% gets used more than 100 times in the month of March.
plot(sort(data$num_clicks_after), (nrow(data):1)/nrow(data), log='xy', type='l', panel.first=grid(),
     xlab='Mininum number of clicks', ylab='Fraction of new links')
frac_1 <- sum(data$num_clicks_after>0)/nrow(data)
abline(h=frac_1)
text(1000, frac_1, sprintf('No clicks: %.0f%%', (1-frac_1)*100), pos=3)
text(1000, frac_1, sprintf('At least one click: %.0f%%', frac_1*100), pos=1)

# Now use p_st on the x axis instead. We want everyone to be able to achieve a given low number (1/N),
# so we restrict ourselves to sources with at least N views. If we also restrict ourselves to the
# range [1/N, 1] on the axis, then every data point can potentially be everywhere along this scale.
# The more pageviews the source has, the harder it is for a new link to achieve a large number of clicks.
# I expect this to be the case because high volume of s is correlated with a large number of outlinks of s.
# TODO: control for p_s, match articles with many and few outlinks, and compare. Alternative hypothesis:
# stopping prob might vary with p_s.
#### Get outdegree from link_positions table.
Min_num_s <- c(1e2, 1e3, 1e4, 1e5)
for (i in 1:length(Min_num_s)) {
  min_num_s <- Min_num_s[i]
  idx <- which(is.finite(data$pst_after) & data$num_source_after >= min_num_s)
  if (i == 1) {
    plot(sort(data$pst_after[idx]), (length(idx):1)/length(idx), log='xy', type='l', col=i,
         panel.first=grid(), xlim=c(1/min_num_s, 1), ylim=c(1e-6, 1),
         xlab='Mininum p_st', ylab='Fraction of new links')
  } else {
    lines(sort(data$pst_after[idx]), (length(idx):1)/length(idx), col=i)
  }
}
legend('topright', legend=Min_num_s, lty=1, col=1:length(Min_num_s))

# Once a link is introduced, it accounts for nearly all traffic from s to t.
hist(data$click_ratio_after, breaks=5, xlab='Direct-click ratio', main='')
```

# Search as a predictor of link usage

```{r}
# Order the candidates by their search proportions before the links are added.
ord_ext <- order(data$search_prob_ext_before, decreasing=TRUE)
ord_wiki <- order(data$search_prob_wiki_before, decreasing=TRUE)
ord_all <- order(data$search_prob_all_before, decreasing=TRUE)
# We don't want to consider those with very few pageviews before, to avoid noisy search prob estimates.
#ord_ext <- ord_all[data$num_source_before[ord_ext] > 100]
#ord_wiki <- ord_all[data$num_source_before[ord_wiki] > 100]
#ord_all <- ord_all[data$num_source_before[ord_all] > 100]

x_wiki <- data$search_prob_wiki_before[ord_wiki]
x_ext <- data$search_prob_ext_before[ord_ext]
x_all <- data$search_prob_all_before[ord_all]

y_wiki <- data$pst_after[ord_wiki]
y_ext <- data$pst_after[ord_ext]
y_all <- data$pst_after[ord_all]

# Randomize the order of the candidates with search scores of 0, to be sure to eliminate any
# external bias among those equally ranked candidates.
idx <- which(x_wiki==0);  y_wiki[idx] <- y_wiki[idx[sample(length(idx), length(idx))]]
idx <- which(x_ext==0);   y_ext[idx] <- y_ext[idx[sample(length(idx), length(idx))]]
idx <- which(x_all==0);   y_all[idx] <- y_all[idx[sample(length(idx), length(idx))]]

plot(x_all, y_all, col='#00000005', pch=16, log='xy')
abline(0,1)

draw_search_rank_vs_usage_smoothed <- function(x, y, type, col) {
  xmax <- 2e4
  smooth <- function(y) ksmooth(1:length(y), y, bandwidth=50, kernel='normal')$y
  plot(y, col='#00000005', pch=16, cex=2, xlim=c(0,xmax), log='y',
       xlab='Rank w.r.t. search proportion (Jan 15)', ylab='Proportion of direct clicks (Mar 15)',
       main=sprintf('%s as a predictor of link quality', type))
  lines(smooth(y), col=col, xlim=c(0,xmax), lwd=2)
  x0 <- min(which(x==0))
  abline(v=x0)
  text(x0/2, 0.8, 'At least\none search')
  text((x0+xmax)/2, 0.8, 'No search')  
}

draw_search_rank_vs_usage_smoothed(x_all, pmax(1e-6, y_all), 'Search', 'black')
draw_search_rank_vs_usage_smoothed(x_wiki, pmax(1e-6, y_wiki), 'Wiki search', 'green')
draw_search_rank_vs_usage_smoothed(x_ext, pmax(1e-6, y_ext), 'External search', 'red')

# Those with very high-ranking search proportion are also more likely to get p_st = 0.
# Hypothesis 1: these are the sources with very few views (but nearly all of them with a search).
# No! It also holds when we have a min threshold on #s before.

draw_search_rank_vs_usage_smoothed(x_all, y_all==0, 'Search', 'black')
sm <- ksmooth(log(data$num_source_before[ord_all]), x_all, bandwidth=2, kernel='normal')
plot(sm$x, sm$y, type='l', log='xy')

# Let's draw the same plots again, but instead of rank, we use the actual value that's used for
# ranking on the x-axis.
draw_search_prop_vs_usage_smoothed <- function(x, y, type, col) {
  smooth <- function(y) ksmooth(1:length(y), y, bandwidth=50, kernel='normal')$y
  plot(x, smooth(y), type='l', col=col, xlim=c(1e-6, 0.1), ylim=c(1e-6, 0.1), log='xy',
       main=sprintf('%s as a predictor of link quality', type),
       xlab='Search proportion (Jan 15)', ylab='Proportion of direct clicks (Mar 15)')
  rug(x, col='#00000050')
  abline(0,1)
#    dens_log <- density(log(x[x>0]))
#    x_dens <- exp(dens_log$x)
#    y_dens <- dens_log$y
#    lines(x_dens, (y_dens-min(y_dens))/(max(y_dens)-min(y_dens))*0.05+0.45)
}

draw_search_prop_vs_usage_smoothed(x_all, y_all, 'Search', 'black')
draw_search_prop_vs_usage_smoothed(x_wiki, y_wiki, 'Wiki search', 'green')
draw_search_prop_vs_usage_smoothed(x_ext, y_ext, 'External search', 'red')

# Next, let's consider a quality measure that always evaluates the entire top of the ranking up to
# a given rank. We plot rank vs. the average direct-click probability of the items above that rank.
draw_search_rank_vs_avg_usage <- function(x, y, type, col) {
  xmax <- 2e4
  plot(cumsum(y[is.finite(y)])/(1:sum(is.finite(y))), type='l', col=col, log='y', xlim=c(0,xmax), #ylim=c(0.005, 0.6),
       main=sprintf('%s as a predictor of link quality', type),
       xlab='Rank w.r.t. search proportion (Jan 15)', ylab='Avg. proportion of direct clicks (Mar 15)')
  x0 <- min(which(x==0))
  abline(v=x0)
  text(x0/2, 0.2, 'At least\none search')
  text((x0+xmax)/2, 0.2, 'No search')  
}

draw_search_rank_vs_avg_usage(x_all, y_all, 'Search', 'black')
draw_search_rank_vs_avg_usage(x_wiki, y_wiki, 'Wiki search', 'green')
draw_search_rank_vs_avg_usage(x_ext, y_ext, 'External search', 'red')
```

# Plot certain source properties vs. p(st)

```{r}
plot_groups <- function(x, y, breaks) {
  idx <- which(is.finite(x))
  means <- unlist(lapply(split(y[idx], cut(x[idx], breaks=breaks)), function(x) mean(x, na.rm=TRUE)))
  groups <- split(y[idx], cut(x[idx], breaks=breaks))
  boxplot(groups, outline=FALSE, ylim=c(0, max(means)))
  lines(means, type='b')
}

plot_groups(data$rel_link_pos, data$pst_after, quantile(data$rel_link_pos, seq(0,1,0.1), na.rm=TRUE))
plot_groups(data$outdegree, data$pst_after, quantile(data$outdegree, seq(0,1,0.1), na.rm=TRUE))
plot_groups(data$source_length, data$pst_after, quantile(data$source_length, seq(0,1,0.1), na.rm=TRUE))
plot_groups(data$num_source_before, data$pst_after, quantile(data$source_length, seq(0,1,0.1), na.rm=TRUE))

plot_groups(data$source_length, data$search_prob_all_before, quantile(data$source_length, seq(0,1,0.1), na.rm=TRUE))

# TODO: control for popularity and check for impact of length

# Important: longer aricles are more popular.
l <- tapply(data$source_length, data$src, unique)
n <- tapply(data$num_source_before, data$src, unique)
plot_groups(l, log10(n), quantile(l, seq(0,1,0.1), na.rm=TRUE))
```

# Load null model and baselines

```{r}
null <- read.table(pipe(sprintf('gunzip -c %s/link_candidates_FREQUENT_WITH-NULL-MODEL_month=1.tsv.gz | cut -f2-7,10-', DATADIR)),
                   header=TRUE, quote='', sep='\t')
rownames(null) <- paste(null$source, null$target)
null$p_indir <- null$num_paths_before/null$s_count
null$p_dir_pred <- pmin(1, pmax(0, (null$p_indir - null$null_model_p) / (1 - null$null_model_p)))
null$prob_label_pred <- pmin(1, pmax(0, null$p_dir_pred / null$p_indir))
null$log_p_indir <- log10(null$p_indir)
null$log_pst <- log10(null$p_s_direct_t)

# Exclude illegal articles
null <- null[rownames(null) %in% data$cand,]

baselines <- read.table(sprintf('%s/filtered_candidates_baselines_v1_month=1.tsv', DATADIR), header=TRUE, quote='', sep='\t')
rownames(baselines) <- paste(baselines$source, baselines$target)
null$source_mean_baseline <- baselines[rownames(null), 'source_mean_baseline']
null$source_all_outlinks_mean_baseline <- baselines[rownames(null), 'source_all_outlinks_mean_baseline']
```

# Output data for global optimization
```{r}
output <- data.frame(
  src=data$src,
  tgt=data$tgt,
  top7k=as.numeric(data$cand %in% rownames(null)),
  src_count=data$num_source_before,
  p_st=data$pst_after,
  p_indir=data$p_indir_before,
  p_search=data$search_prob_all_before,
  p_null=NA,
  p_mean_taken=NA,
  p_mean_all=NA
  )
rownames(output) <- data$cand
output$p_null[rownames(output) %in% rownames(null)] <- null$null_model_p
output$p_mean_taken[rownames(output) %in% rownames(null)] <- null$source_mean_baseline
output$p_mean_all[rownames(output) %in% rownames(null)] <- null$source_all_outlinks_mean_baseline

write.table(output, sprintf('%s/global_optimization_input.tsv', DATADIR), quote=FALSE, sep='\t', row.names=FALSE)
```

```{r}
#######################################################################
# Only 2% of data points have p_st = 0, so we may ignore them for taking logs.
sum(null$p_s_direct_t==0)/nrow(null)
null <- null[null$p_s_direct_t > 0,]

rel_diff <- function(pred, true) abs(true-pred)#/true

# Stupid baseline.
plot(null$source_mean_baseline, null$p_s_direct_t, log='xy', col='#00000010', pch=16, panel.first=c(abline(0,1)), xlim=c(1e-5,1), ylim=c(1e-5,1))
x <- sort(rel_diff(null$source_mean_baseline, null$p_s_direct_t))
summary(x)
plot(x[is.finite(x)], (1:sum(is.finite(x)))/sum(is.finite(x)), type='l', panel.first=grid(), log='xy', xlim=c(1e-4,1e2))

# p_indir.
plot(null$log_p_indir, null$log_pst, col='#00000010', pch=16, panel.first=c(abline(0,1)), xlim=c(-5,0), ylim=c(-5,0))

plot(x[is.finite(x)], (1:sum(is.finite(x)))/sum(is.finite(x)), type='l', panel.first=grid(), log='xy', xlim=c(1e-4,1e2))

x <- sort(abs(null$p_indir - null$p_s_direct_t))
plot(x, (1:sum(is.finite(x)))/sum(is.finite(x)), log='x', type='l', panel.first=grid())

coef <- coef(lm(null$log_pst ~ null$log_p_indir + I(null$log_p_indir^2)))
plot(null$log_p_indir, null$log_pst, col='#00000010', pch=16, panel.first=c(abline(0,1)))
curve(coef[1] + x*coef[2] + x^2*coef[3], from=-14, to=0, add=TRUE, col='green', lwd=4)

plot(density(log10(null$p_indir) - log10(null$p_s_direct_t)))

plot(density(log10(null$p_s_direct_t)), panel.first=grid())
quantile(log10(null$p_s_direct_t), c(0.025, 0.05, 0.1, 0.9, 0.95, 0.975))

# Sources are quite frequent.
plot(density(log10(null$s_count)))
plot(sort(null$s_count), (nrow(null):1)/nrow(null), log='x', type='l', panel.first=grid())

# TODO: Macro-avg by source
#######################################################################


# null_3 <- read.table(pipe(sprintf('gunzip -c %s/link_candidates_FREQUENT_WITH-NULL-MODEL_month=3.tsv.gz | cut -f2-7,10-', DATADIR)),
#                    header=TRUE, quote='', sep='\t')
# rownames(null_3) <- paste(null_3$source, null_3$target)
# common <- intersect(rownames(null), rownames(null_3))
# plot(null$null_model_p[rownames(null) %in% common], null_3$null_model_p[rownames(null_3) %in% common], log='xy', xlim=c(10e-7,1), ylim=c(10e-7,1))

abline(0, 1, col='red')
smooth <- function(x, y) ksmooth(x, y, bandwidth=0.05, kernel='normal')
ord <- order(null$prob_label_pred)
sm <- smooth(null$prob_label_pred[ord], null$prob_label[ord])
lines(sm$x, sm$y)
rug(null$prob_label_pred[ord], col='#00000010')
dens <- density(null$prob_label_pred[ord])
lines(dens$x, dens$y/5, bw='SJ')
abline(0, 1, col='red')

plot(null$prob_label_pred, null$prob_label, pch=16, col='#00000010')

smooth <- function(y) ksmooth(1:length(y), y, bandwidth=100, kernel='normal')
sm <- smooth(null$prob_label[ord])
plot(sm$x, sm$y, type='l')
abline(v=max(which(null$prob_label_pred[ord]==0)))

head(null[rev(ord),])

plot(null$p_dir_pred, null$p_s_direct_t, log='xy', xlim=c(1e-6,1), ylim=c(1e-6,1), pch=16, col='#00000010')
abline(0, 1, col='red')

plot(null$p_indir, null$p_s_direct_t, log='xy', xlim=c(1e-6,1), ylim=c(1e-6,1), pch=16, col='#00000010')
abline(0, 1, col='red')

plot(null$p_dir_pred, null$null_model_p, log='xy', xlim=c(1e-6,1), ylim=c(1e-6,1), pch=16, col='#00000010')
abline(0, 1, col='red')

pairs(null[,c('p_s_direct_t', 'p_dir_pred', 'null_model_p', 'p_indir', 'p_indir_pred')],
      panel=function(...) points(..., panel.first=abline(0,1,col='red')),
      log='xy', pch=16, col='#00000010', xlim=c(1e-6,1), ylim=c(1e-6,1))


# Check if we can rank meaningfully even when source is fixed.
ord <- order(null$p_indir - null$null_model_p, decreasing=TRUE)
counts <- tapply(null$source, null$source, length)
nn <- null[null$source %in% names(counts)[counts >= 10],]
summary(by(nn, nn$source, function(n) cor(n$p_indir, n$p_s_direct_t, method='spearman')))
head(null[ord,c('p_s_direct_t', 'p_indir', 'null_model_p')])

plot(null$p_s_direct_t, null$p_indir - null$null_model_p, log='xy', xlim=c(1e-6,1), ylim=c(1e-6,1))

plot(null$p_s_direct_t, null$p_indir, log='xy', xlim=c(1e-6,1), ylim=c(1e-6,1))
abline(0,1,col='red')

#####################################
xx <- log10(null$null_model_p[null$null_model_p > 0 & null$p_s_direct_t > 0])
yy <- log10(null$p_s_direct_t[null$null_model_p > 0 & null$p_s_direct_t > 0])
plot(xx, yy, col='#00000005', pch=16, cex=2, xlim=range(c(xx,yy)), ylim=range(c(xx,yy)))
abline(0, 1, col='green')
coef <- coef(lm(yy ~ xx + I(xx^2)))
curve(coef[1] + x*coef[2] + x^2*coef[3], from=-14, to=0, add=TRUE, col='red', lwd=2)

xx <- log10(null$p_indir[null$p_indir > 0 & null$p_s_direct_t > 0])
yy <- log10(null$p_s_direct_t[null$p_indir > 0 & null$p_s_direct_t > 0])
plot(xx, yy, col='#00000005', pch=16, cex=2, xlim=range(c(xx,yy)), ylim=range(c(xx,yy)))
abline(0, 1, col='green')
coef <- coef(lm(yy ~ xx + I(xx^2)))
curve(coef[1] + x*coef[2] + x^2*coef[3], from=-14, to=0, add=TRUE, col='red', lwd=2)

xx <- log10(null$p_dir_pred[null$p_dir_pred > 0 & null$p_s_direct_t > 0])
yy <- log10(null$p_s_direct_t[null$p_dir_pred > 0 & null$p_s_direct_t > 0])
plot(xx, yy, col='#00000005', pch=16, cex=2, xlim=c(-6,0), ylim=c(-6,0))
abline(0, 1)
coef <- coef(lm(yy ~ xx + I(xx^2)))
smooth <- function(x, y) ksmooth(x, y, bandwidth=0.05, kernel='normal')
lines(smooth(xx,yy)$x, smooth(xx,yy)$y, col='orange', lwd=3)
curve(coef[1] + x*coef[2] + x^2*coef[3], from=-14, to=0, add=TRUE, col='green', lwd=4)
abline(v=quantile(xx, c(.025,.975)))

f <- function(x, coef) coef[1] + x*coef[2] + x^2*coef[3]
plot(f(xx, coef), yy, col='#00000005', pch=16, cex=2, xlim=c(-4,-1), ylim=c(-4,-1))
abline(0, 1)
abline(v=f(quantile(xx, c(.025,.975)), coef))

# Evaluate the different predictions.
null$p_dir_pred_boosted <- 10^(f(log10(null$p_dir_pred), coef))
null$delta <- null$p_indir - null$null_model_p
null$mean <- (null$p_indir + null$null_model_p)/2
idx <- which(null$null_model_p > 0 & null$ratio > 0 & null$p_s_direct_t > 0)
#idx <- which(null$p_dir_pred > 0 & null$null_model_p > 0 & null$p_indir > 0 & null$p_s_direct_t > 0)
#idx <- which(null$null_model_p + null$p_indir > 0 & null$p_s_direct_t > 0)
d <- null[idx, c('p_dir_pred', 'p_dir_pred_boosted', 'null_model_p', 'p_indir', 'delta', 'mean', 'p_s_direct_t')]
cor(log(d), use='complete.obs')
cor(log(d), method='spearman', use='complete.obs')
dist(t(log(d)), method='manhattan')
dist(t(log(d)), method='euclidean')

logl <- outer(1:7, 1:7, FUN=Vectorize( function(i,j) -sum(d[,i]*log(d[,j]) + (1-d[,i])*log(1-d[,j])) ))
rownames(logl) <- colnames(logl) <- c('p_dir_pred', 'p_dir_pred_boosted', 'null_model_p', 'p_indir', 'delta', 'mean', 'p_s_direct_t')
logl

input <- d
input$log_p_star <- log(input$p_indir)
input$log_p_null <- log(input$null_model_p)
input$prod <- log(input$p_indir)*log(input$null_model_p)

glm <- glm(p_s_direct_t ~ log_p_star + log_p_null + prod, family='binomial', data=input)
plot(predict.glm(glm, input, type='response'), input$p_s_direct_t, log='xy', xlim=c(1e-6,1), ylim=c(1e-6,1))
abline(0,1,col='red')


pst <- 10^(f(log10(null$p_dir_pred), coef))
z = pst / (pst + (1-pst)*null$null_model_p)
plot(sort(z))

smooth <- function(x, y) ksmooth(x, y, bandwidth=0.01, kernel='normal')
sm <- smooth(z[is.finite(z)], null$prob_label[is.finite(z)])
plot(z, null$prob_label, pch=16, col='#00000010')
lines(sm$x, sm$y, lwd=2, col='red')

ord <- order(z, na.last=FALSE)
smooth <- function(x, y) ksmooth(x, y, bandwidth=100, kernel='normal')
sm <- smooth(1:length(ord), null$prob_label[ord])
plot(sm$x, sm$y, type='l')
par(new=TRUE)
plot(1:length(ord), z[ord], type='l', col='red')
par(.default_par)

par(mfrow=c(2,1))
plot(1:length(ord), null$null_model_p[ord])
plot(1:length(ord), null$p_indir[ord], col='red')
plot(1:length(ord), null$p_dir_pred[ord], col='red', type='l')
par(.default_par)
plot(1:length(ord), z[ord], ylim=c(0,5))

## null[400,]: num_paths_before > s_count

#####################################

```













```{r}
null <- read.table(pipe(sprintf('gunzip -c %s/link_candidates_FREQUENT_WITH-NULL-MODEL_month=1.tsv.gz | cut -f2-7,10-', DATADIR)),
                   header=TRUE, quote='', sep='\t')
rownames(null) <- paste(null$source, null$target)
null$p_indir <- null$num_paths_before/null$s_count
null$log_p_indir <- log10(null$p_indir)
null$log_pst <- log10(null$p_s_direct_t)

# Exclude illegal articles
null <- null[rownames(null) %in% data$cand,]

input <- data[data$cand %in% rownames(null),]
rownames(input) <- rownames(null)

# Only 2% of data points have p_st = 0, so we may ignore them for taking logs.
sum(input$pst_after==0)/nrow(input)
input <- input[input$pst_after > 0,]

baselines <- read.table(sprintf('%s/filtered_candidates_baselines_v1_month=1.tsv', DATADIR), header=TRUE, quote='', sep='\t')
rownames(baselines) <- paste(baselines$source, baselines$target)
input$source_mean_baseline <- baselines[rownames(input), 'source_mean_baseline']
input$source_all_outlinks_mean_baseline <- baselines[rownames(input), 'source_all_outlinks_mean_baseline']

idx <- sample(nrow(input), floor(nrow(input)/2))
train <- input[idx,]
test <- input[setdiff(1:nrow(input), idx),]

train_and_test <- function(formula) {
  #model <- glm(formula, family=quasibinomial, data=train)
  #pred <- predict(model, type='response', newdata=test)
  #model <- gbm(formula, distribution='laplace', data=train, n.trees=100, interaction.depth=4, bag.fraction=0.5, cv.folds=3, n.cores=1)
  #model <- gbm(formula, data=train, distribution='laplace', n.trees=100, interaction.depth=4, bag.fraction=0.5, cv.folds=3)
  model <- lm(formula, data=train)
  pred <- predict(model, newdata=test)
  true <- test$pst_after
  absdiff <- summary(abs(true-pred))
  reldiff <- summary(abs(true-pred)/true)
  list(absdiff=absdiff, reldiff=reldiff, pred=pred, true=true, model=model)
}

train_and_test(pst_after ~ 1)

train_and_test(pst_after ~ log10(source_mean_baseline))
train_and_test(pst_after ~ log10(source_mean_baseline) + log10(p_indir_before))
train_and_test(pst_after ~ log10(source_mean_baseline) + log10(p_indir_before) + log10(pmax(search_prob_all_before, 1e-7)))

res <- train_and_test(pst_after ~ #source_mean_baseline
                                   p_indir_before
                                   #+ search_prob_all_before
                                   #+ rel_link_pos
                                   #+ source_length
                      )
res$absdiff
plot(res$pred, res$true, log='xy', pch=16, col='#00000010'); abline(0,1)

```
